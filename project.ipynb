{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. INSTALL NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (0.3.21)\n",
      "Requirement already satisfied: discord.py in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (2.5.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (1.10.0)\n",
      "Requirement already satisfied: pypdf in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (5.4.0)\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (1.26.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.51.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1+cu124)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\python312\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_community discord.py sentence-transformers faiss-cpu pypdf llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. IMPORT NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import discord\n",
    "from discord.ext import commands\n",
    "import pickle\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SET API KEYS AND DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DISCORD_TOKEN\"] = \"\"\n",
    "PDF_DIRECTORY = \"algorithms_docs\"\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LOAD PDFS WITH METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_metadata(directory_path):\n",
    "    documents = []\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(directory_path, file)\n",
    "            try:\n",
    "                # Determine document type from filename or folder structure\n",
    "                doc_type = \"unknown\"\n",
    "                if \"lecture\" in file.lower():\n",
    "                    doc_type = \"lecture\"\n",
    "                elif \"hw\" in file.lower():\n",
    "                    if \"sol\" in file.lower():\n",
    "                        doc_type = \"homework_solution\"\n",
    "                    else:\n",
    "                        doc_type = \"homework\"\n",
    "                elif \"review\" in file.lower():\n",
    "                    doc_type = \"exam\"\n",
    "                elif \"midterm\" in file.lower():\n",
    "                    doc_type = \"exam\"\n",
    "                elif \"practice\" in file.lower():\n",
    "                    doc_type = \"practice_problem\"\n",
    "                    \n",
    "                # Load PDF\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                    \n",
    "                    # Add metadata to each page\n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_type\"] = doc_type\n",
    "                    doc.metadata[\"filename\"] = file\n",
    "                    \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded: {file} as {doc_type}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Final Review.pdf as exam\n",
      "Loaded: HW0.pdf as homework\n",
      "Loaded: HW01.pdf as homework\n",
      "Loaded: HW01_sol.pdf as homework_solution\n",
      "Loaded: HW02.pdf as homework\n",
      "Loaded: HW02_sol.pdf as homework_solution\n",
      "Loaded: HW03.pdf as homework\n",
      "Loaded: HW03_sol.pdf as homework_solution\n",
      "Loaded: HW04.pdf as homework\n",
      "Loaded: HW04_sol.pdf as homework_solution\n",
      "Loaded: HW05.pdf as homework\n",
      "Loaded: HW05_sol.pdf as homework_solution\n",
      "Loaded: HW06.pdf as homework\n",
      "Loaded: HW06_sol.pdf as homework_solution\n",
      "Loaded: HW07.pdf as homework\n",
      "Loaded: HW07_sol.pdf as homework_solution\n",
      "Loaded: HW08.pdf as homework\n",
      "Loaded: HW08_sol.pdf as homework_solution\n",
      "Loaded: HW09.pdf as homework\n",
      "Loaded: HW09_sol.pdf as homework_solution\n",
      "Loaded: HW10.pdf as homework\n",
      "Loaded: HW10_sol.pdf as homework_solution\n",
      "Loaded: Lecture 0.pdf as lecture\n",
      "Loaded: Lecture 01.pdf as lecture\n",
      "Loaded: Lecture 02.pdf as lecture\n",
      "Loaded: Lecture 03.pdf as lecture\n",
      "Loaded: Lecture 04.pdf as lecture\n",
      "Loaded: Lecture 05.pdf as lecture\n",
      "Loaded: Lecture 06.pdf as lecture\n",
      "Loaded: Lecture 07.pdf as lecture\n",
      "Loaded: Lecture 08.pdf as lecture\n",
      "Loaded: Lecture 09.pdf as lecture\n",
      "Loaded: Lecture 10.pdf as lecture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Lecture 11.pdf as lecture\n",
      "Loaded: Lecture 12.pdf as lecture\n",
      "Loaded: Lecture 13.pdf as lecture\n",
      "Loaded: Midterm Review.pdf as exam\n",
      "Loaded: MidtermA.pdf as exam\n",
      "Loaded: MidtermB.pdf as exam\n",
      "Loaded: PracticeProblems1.pdf as practice_problem\n",
      "Loaded: PracticeProblems2.pdf as practice_problem\n",
      "Loaded: PracticeProblems3.pdf as practice_problem\n",
      "Loaded: PracticeProblems4.pdf as practice_problem\n",
      "Loaded 1014 document pages in total\n"
     ]
    }
   ],
   "source": [
    "documents = load_pdfs_metadata(PDF_DIRECTORY)\n",
    "print(f\"Loaded {len(documents)} document pages in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types distribution:\n",
      "- exam: 128 pages\n",
      "- homework: 24 pages\n",
      "- homework_solution: 43 pages\n",
      "- lecture: 719 pages\n",
      "- practice_problem: 100 pages\n"
     ]
    }
   ],
   "source": [
    "doc_types = {}\n",
    "for doc in documents:\n",
    "    doc_type = doc.metadata.get(\"source_type\", \"unknown\")\n",
    "    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "\n",
    "print(\"Document types distribution:\")\n",
    "for doc_type, count in doc_types.items():\n",
    "    print(f\"- {doc_type}: {count} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first document (Final Review.pdf):\n",
      "Final Review\n",
      "greedy algorithms\n",
      "divide and conquer\n",
      "dynamic programming\n",
      "CS 3330 Algorithms\n"
     ]
    }
   ],
   "source": [
    "if documents:\n",
    "    print(f\"Preview of the first document ({documents[0].metadata[\"filename\"]}):\")\n",
    "    preview_text = documents[0].page_content[:500] + \"...\" if len(documents[0].page_content) > 500 else documents[0].page_content\n",
    "    print(preview_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. CREATE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1560 chunks from 1014 document pages\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} document pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example chunk (fromFinal Review.pdf):\n",
      "\n",
      "Final Review\n",
      "greedy algorithms\n",
      "divide and conquer\n",
      "dynamic programming\n",
      "CS 3330 Algorithms\n",
      "\n",
      "Chunk metadata: {'producer': 'Adobe PDF Library 24.4.48', 'creator': 'Acrobat PDFMaker 24 for PowerPoint', 'creationdate': '2024-12-12T08:01:47-06:00', 'author': 'Moharrami, Mehrdad', 'company': 'University of Iowa', 'moddate': '2024-12-12T08:01:54-06:00', 'title': 'PowerPoint Presentation', 'source': 'algorithms_docs\\\\Final Review.pdf', 'total_pages': 55, 'page': 0, 'page_label': '1', 'source_type': 'exam', 'filename': 'Final Review.pdf'}\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    print(f\"Example chunk (from{chunks[0].metadata[\"filename\"]}):\\n\")\n",
    "    preview_chunk = chunks[0].page_content[:300] + \"...\" if len(chunks[0].page_content) > 300 else chunks[0].page_content\n",
    "    print(preview_chunk)\n",
    "    print(\"\\nChunk metadata:\", chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. CREATE VECTOR STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and saved to faiss_index\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding = embeddings)\n",
    "vectorstore.save_local(FAISS_INDEX_PATH)\n",
    "print(f\"Vector store created and saved to {FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. RETRIEVER FOR FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_filter(doc_type):\n",
    "    def filter_func(doc):\n",
    "        return doc.metadata.get(\"source_type\") == doc_type\n",
    "    return filter_func\n",
    "\n",
    "def create_retriever(doc_type = None):\n",
    "    vs = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    if doc_type:\n",
    "        base_retriever = vs.as_retriever(search_kwargs={\"k\": 10})\n",
    "        def filtered_retriever(query):\n",
    "                docs = base_retriever.invoke(query)\n",
    "                filtered_docs = [doc for doc in docs if doc.metadata.get(\"source_type\") == doc_type]\n",
    "                return filtered_docs[:4]\n",
    "        return filtered_retriever\n",
    "    else:\n",
    "        return vs.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. SET UP LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM setup completed\n"
     ]
    }
   ],
   "source": [
    "def setup_llm():\n",
    "    return HuggingFaceHub(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",  # You can change to another model\n",
    "        model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1024, \"n_ctx\": 2048, \"verbose\": True},\n",
    "        huggingfacehub_api_token=os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    )\n",
    "\n",
    "# Try to setup the LLM\n",
    "try:\n",
    "    llm = setup_llm()\n",
    "    print(\"LLM setup completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. CREATE RAG CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are an algorithms teaching assistant for a computer science class.\n",
    "Answer the question based only on the following context from class materials:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "When answering:\n",
    "1. Be thorough and explain concepts clearly like a teaching assistant would\n",
    "2. Use examples to illustrate complex algorithms when appropriate\n",
    "3. Include time and space complexity analysis when relevant\n",
    "4. If the answer isn't fully contained in the context, say so rather than making up information\n",
    "5. Don't hallucinate\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever = create_retriever()\n",
    "\n",
    "def create_rag_chain(retriever_func):\n",
    "    if callable(retriever_func) and not hasattr(retriever_func, 'invoke'):\n",
    "        retriever_chain = RunnablePassthrough() | retriever_func | format_docs\n",
    "    else:\n",
    "        retriever_chain = retriever_func | format_docs\n",
    "    \n",
    "    chain = (\n",
    "        {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "rag_chain = create_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. SMART DOCUMENT SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_query(question):\n",
    "    question_lower = question.lower()\n",
    "    if any(term in question_lower for term in [\"solution\", \"answer\", \"solved\", \"how to solve\"]):\n",
    "        print(\"Using primarily homework solutions for this query...\")\n",
    "        custom_retriever = create_retriever(\"homework_solution\")\n",
    "    elif any(term in question_lower for term in [\"lecture\", \"class\", \"taught\", \"professor\"]):\n",
    "        print(\"Using primarily lecture materials for this query...\")\n",
    "        custom_retriever = create_retriever(\"lecture\")\n",
    "    elif any(term in question_lower for term in [\"exam\", \"test\", \"quiz\", \"midterm\", \"final\"]):\n",
    "        print(\"Using primarily exam materials for this query...\")\n",
    "        custom_retriever = create_retriever(\"exam\")\n",
    "    elif any(term in question_lower for term in [\"homework\", \"assignment\", \"problem set\"]):\n",
    "        print(\"Using primarily homework materials for this query...\")\n",
    "        custom_retriever = create_retriever(\"homework\")\n",
    "    else:\n",
    "        print(\"Using all course materials for this query...\")\n",
    "        custom_retriever = create_retriever()\n",
    "    temp_chain = create_rag_chain(custom_retriever)\n",
    "    return temp_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. TEST RAG SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all course materials for this query...\n",
      "Question: Explain how merge-sort works\n",
      "\n",
      "Answer:\n",
      "Human: You are an algorithms teaching assistant for a computer science class.\n",
      "Answer the question based only on the following context from class materials:\n",
      "\n",
      "Mergesort implementation\n",
      "tnputTFFwistFLFofFn elementsFfromFaFtotallyForderedFuniverseTF\n",
      "zutputTFF—heFnFelementsFinFascendingForderT\n",
      "c\n",
      "MERGE-SORT(L)\n",
      "\n",
      "8\n",
      "Divide and conquer: merge sort\n",
      "\n",
      "7\n",
      "Divide and conquer: merge sort\n",
      "\n",
      "Mergesort\n",
      "独”ecursivelyFsortFleftFhalfTF\n",
      "独”ecursivelyFsortFrightFhalfTF\n",
      "独xergeFtwoFhalvesFtoFmakeFsortedFwholeT\n",
      "a\n",
      "l r s t w x z ” … —\n",
      "merge results\n",
      "l w r z ” t — s x …\n",
      "input\n",
      "t — s x …l r w z ”\n",
      "sort left half\n",
      "s t x … —\n",
      "sort right half\n",
      "l r w z ”\n",
      "\n",
      "Question: Explain how merge-sort works\n",
      "\n",
      "When answering:\n",
      "1. Be thorough and explain concepts clearly like a teaching assistant would\n",
      "2. Use examples to illustrate complex algorithms when appropriate\n",
      "3. Include time and space complexity analysis when relevant\n",
      "4. If the answer isn't fully contained in the context, say so rather than making up information\n",
      "5. Don't hallucinate\n",
      "\n",
      "Answer: Merge-sort is a divide and conquer algorithm, which means it solves a problem by breaking it down into smaller sub-problems. In the context of merge-sort, the problem is to sort an array or a list of elements.\n",
      "\n",
      "The merge-sort algorithm works as follows:\n",
      "\n",
      "1. Divide: The input list or array is divided into two halves. This is done recursively, meaning that each half is also divided into two halves, and so on, until we reach lists or arrays with only one element. These single-element lists are considered sorted.\n",
      "\n",
      "2. Conquer: Each half of the list is sorted recursively using the merge-sort algorithm. This is where the \"sorted\" halves come from in the context.\n",
      "\n",
      "3. Merge: Once both halves are sorted, they are merged back together to form a single sorted list. This is done by comparing the first elements of both lists and putting the smaller one at the beginning of the merged list. This process continues until one of the lists is empty, at which point the remaining elements from the other list are added to the merged list.\n",
      "\n",
      "For example, let's say we want to sort the list [5, 3, 8, 6, 1, 9, 4, 7]. The first step would be to divide the list into two halves: [5, 3, 6, 1] and [8, 9, 4, 7]. Both halves are then sorted recursively. The left half becomes [1, 3, 5, 6] and the right half becomes [4, 7, 8, 9]. Now, the two sorted halves are merged back together to form the final sorted list: [1, 3, 5, 6, 4, 7, 8, 9].\n",
      "\n",
      "As for the time and space complexity, merge-sort has a time complexity of O(n log n) in the average and worst cases, where n is the number of elements in the list. This is because the divide and conquer step takes O(log n) time on average, and the merge step takes O(n) time in the worst case when the lists being merged are of equal size. The space complexity is O(n) in the worst case, as we need to store the entire input list during the recursion. However, in practice, the space complexity is often O(log n) due to the use of in-place merging.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Explain how merge-sort works\"\n",
    "try:\n",
    "    answer = smart_query(test_question)\n",
    "    print(\"Question:\", test_question)\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nYou may need to download a language model or start text-generation-webui.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. CREATE DISCORD BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mAlgorithmsBot\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mcommands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiscord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIntents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "class AlgorithmsBot(commands.bot):\n",
    "    def __init__(self):\n",
    "        intents = discord.Intents.default()\n",
    "        intents.message_content = True\n",
    "        super().__init__(command_prefix = \"!\", intent = intents)\n",
    "    async def on_ready(self):\n",
    "        print(f'{self.user} has connected to Discord!')\n",
    "        print(f'Bot is in {len(self.guilds)} servers')\n",
    "    async def setup_hook(self):\n",
    "        await self.add_cog(AlgorithmsCog())\n",
    "class AlgorithmsCog(commands.Cog):\n",
    "    def __init__(self):\n",
    "        self.retriever = create_retriever()\n",
    "    @commands.command(name='algo')\n",
    "    async def algo_command(self, ctx, *, question):\n",
    "        async with ctx.typing():\n",
    "            try:\n",
    "                answer = smart_query(question)\n",
    "                if len(answer) > 1900:\n",
    "                    chunks = [answer[i:i+1900] for i in range(0, len(answer), 1900)]\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        if i == 0:\n",
    "                            await ctx.send(f\"**Question:** {question}\\n\\n{chunk}\")\n",
    "                        else:\n",
    "                            await ctx.send(chunk)\n",
    "                else:\n",
    "                    await ctx.send(f\"**Question:** {question}\\\\n\\\\n{answer}\")\n",
    "            except Exception as e:\n",
    "                await ctx.send(f\"Error: {str(e)}\")\n",
    "\n",
    "    @commands.command(name='sources')\n",
    "    async def source_filter_command(self, ctx, source_type, *, question):\n",
    "        valid_sources = [\"lecture\", \"homework\", \"solution\", \"exam\", \"all\"]\n",
    "        if source_type.lower() not in valid_sources:\n",
    "            await ctx.send(f\"Invalid source type. Use one of: {', '.join(valid_sources)}\")\n",
    "            return\n",
    "        doc_type = None\n",
    "        if source_type.lower() == \"solution\":\n",
    "            doc_type = \"homework_solution\"\n",
    "        elif source_type.lower() != \"all\":\n",
    "            doc_type = source_type.lower()\n",
    "        async with ctx.typing():\n",
    "            try:\n",
    "                custom_retriever = create_retriever(doc_type)\n",
    "                temp_chain = create_rag_chain(custom_retriever)\n",
    "                answer = temp_chain.invoke(question)\n",
    "                source_display = \"all sources\" if source_type.lower() == \"all\" else f\"{source_type} materials\"\n",
    "                if len(answer) > 1850:\n",
    "                    chunks = [answer[i:i+1850] for i in range(0, len(answer), 1850)]\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        if i == 0:\n",
    "                            await ctx.send(f\"**Question:** {question}\\n**Source:** {source_display}\\n\\n{chunk}\")\n",
    "                        else:\n",
    "                            await ctx.send(chunk)\n",
    "                else:\n",
    "                    await ctx.send(f\"**Question:** {question}\\n**Source:** {source_display}\\n\\n{answer}\")\n",
    "            except Exception as e:\n",
    "                await ctx.send(f\"Error: {str(e)}\")\n",
    "    @commands.command(name='help_algo')\n",
    "    async def help_command(self, ctx):      \n",
    "        help_text = (\n",
    "            \"**Algorithms Bot Commands:**\\n\\n\"\n",
    "            \"`!algo [question]` - Ask any algorithms question\\n\"\n",
    "            \"`!sources [type] [question]` - Search only specific source types\\n\"\n",
    "            \"  - Valid types: lecture, homework, solution, exam, all\\n\"\n",
    "            \"`!help_algo` - Show this help message\\n\\n\"\n",
    "            \"**Examples:**\\n\"\n",
    "            \"`!algo How does quicksort work?`\\n\"\n",
    "            \"`!sources lecture What is dynamic programming?`\"\n",
    "        )\n",
    "        await ctx.send(help_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
