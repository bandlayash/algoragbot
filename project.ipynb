{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. INSTALL NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: langchain in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (0.3.21)\n",
      "Requirement already satisfied: discord.py in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (2.5.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (1.10.0)\n",
      "Requirement already satisfied: pypdf in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (5.4.0)\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (0.3.8)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (1.1.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (1.6.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain_community) (1.26.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.51.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1+cu124)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\python312\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yb200\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_community discord.py sentence-transformers faiss-cpu pypdf llama-cpp-python python-dotenv nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. IMPORT NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import discord\n",
    "from discord.ext import commands\n",
    "import pickle\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SET API KEYS AND DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PDF_DIRECTORY = \"algorithms_docs\"\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "DISCORD_TOKEN = os.getenv(\"DISCORD_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LOAD PDFS WITH METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_metadata(directory_path):\n",
    "    documents = []\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(directory_path, file)\n",
    "            try:\n",
    "                # Determine document type from filename or folder structure\n",
    "                doc_type = \"unknown\"\n",
    "                if \"lecture\" in file.lower():\n",
    "                    doc_type = \"lecture\"\n",
    "                elif \"hw\" in file.lower():\n",
    "                    if \"sol\" in file.lower():\n",
    "                        doc_type = \"homework_solution\"\n",
    "                    else:\n",
    "                        doc_type = \"homework\"\n",
    "                elif \"review\" in file.lower():\n",
    "                    doc_type = \"exam\"\n",
    "                elif \"midterm\" in file.lower():\n",
    "                    doc_type = \"exam\"\n",
    "                elif \"practice\" in file.lower():\n",
    "                    doc_type = \"practice_problem\"\n",
    "                    \n",
    "                # Load PDF\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                    \n",
    "                    # Add metadata to each page\n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_type\"] = doc_type\n",
    "                    doc.metadata[\"filename\"] = file\n",
    "                    \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded: {file} as {doc_type}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Final Review.pdf as exam\n",
      "Loaded: HW0.pdf as homework\n",
      "Loaded: HW01.pdf as homework\n",
      "Loaded: HW01_sol.pdf as homework_solution\n",
      "Loaded: HW02.pdf as homework\n",
      "Loaded: HW02_sol.pdf as homework_solution\n",
      "Loaded: HW03.pdf as homework\n",
      "Loaded: HW03_sol.pdf as homework_solution\n",
      "Loaded: HW04.pdf as homework\n",
      "Loaded: HW04_sol.pdf as homework_solution\n",
      "Loaded: HW05.pdf as homework\n",
      "Loaded: HW05_sol.pdf as homework_solution\n",
      "Loaded: HW06.pdf as homework\n",
      "Loaded: HW06_sol.pdf as homework_solution\n",
      "Loaded: HW07.pdf as homework\n",
      "Loaded: HW07_sol.pdf as homework_solution\n",
      "Loaded: HW08.pdf as homework\n",
      "Loaded: HW08_sol.pdf as homework_solution\n",
      "Loaded: HW09.pdf as homework\n",
      "Loaded: HW09_sol.pdf as homework_solution\n",
      "Loaded: HW10.pdf as homework\n",
      "Loaded: HW10_sol.pdf as homework_solution\n",
      "Loaded: Lecture 0.pdf as lecture\n",
      "Loaded: Lecture 01.pdf as lecture\n",
      "Loaded: Lecture 02.pdf as lecture\n",
      "Loaded: Lecture 03.pdf as lecture\n",
      "Loaded: Lecture 04.pdf as lecture\n",
      "Loaded: Lecture 05.pdf as lecture\n",
      "Loaded: Lecture 06.pdf as lecture\n",
      "Loaded: Lecture 07.pdf as lecture\n",
      "Loaded: Lecture 08.pdf as lecture\n",
      "Loaded: Lecture 09.pdf as lecture\n",
      "Loaded: Lecture 10.pdf as lecture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Lecture 11.pdf as lecture\n",
      "Loaded: Lecture 12.pdf as lecture\n",
      "Loaded: Lecture 13.pdf as lecture\n",
      "Loaded: Midterm Review.pdf as exam\n",
      "Loaded: MidtermA.pdf as exam\n",
      "Loaded: MidtermB.pdf as exam\n",
      "Loaded: PracticeProblems1.pdf as practice_problem\n",
      "Loaded: PracticeProblems2.pdf as practice_problem\n",
      "Loaded: PracticeProblems3.pdf as practice_problem\n",
      "Loaded: PracticeProblems4.pdf as practice_problem\n",
      "Loaded 1014 document pages in total\n"
     ]
    }
   ],
   "source": [
    "documents = load_pdfs_metadata(PDF_DIRECTORY)\n",
    "print(f\"Loaded {len(documents)} document pages in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types distribution:\n",
      "- exam: 128 pages\n",
      "- homework: 24 pages\n",
      "- homework_solution: 43 pages\n",
      "- lecture: 719 pages\n",
      "- practice_problem: 100 pages\n"
     ]
    }
   ],
   "source": [
    "doc_types = {}\n",
    "for doc in documents:\n",
    "    doc_type = doc.metadata.get(\"source_type\", \"unknown\")\n",
    "    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "\n",
    "print(\"Document types distribution:\")\n",
    "for doc_type, count in doc_types.items():\n",
    "    print(f\"- {doc_type}: {count} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first document (Final Review.pdf):\n",
      "Final Review\n",
      "greedy algorithms\n",
      "divide and conquer\n",
      "dynamic programming\n",
      "CS 3330 Algorithms\n"
     ]
    }
   ],
   "source": [
    "if documents:\n",
    "    print(f\"Preview of the first document ({documents[0].metadata[\"filename\"]}):\")\n",
    "    preview_text = documents[0].page_content[:500] + \"...\" if len(documents[0].page_content) > 500 else documents[0].page_content\n",
    "    print(preview_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. CREATE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1560 chunks from 1014 document pages\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} document pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example chunk (fromFinal Review.pdf):\n",
      "\n",
      "Final Review\n",
      "greedy algorithms\n",
      "divide and conquer\n",
      "dynamic programming\n",
      "CS 3330 Algorithms\n",
      "\n",
      "Chunk metadata: {'producer': 'Adobe PDF Library 24.4.48', 'creator': 'Acrobat PDFMaker 24 for PowerPoint', 'creationdate': '2024-12-12T08:01:47-06:00', 'author': 'Moharrami, Mehrdad', 'company': 'University of Iowa', 'moddate': '2024-12-12T08:01:54-06:00', 'title': 'PowerPoint Presentation', 'source': 'algorithms_docs\\\\Final Review.pdf', 'total_pages': 55, 'page': 0, 'page_label': '1', 'source_type': 'exam', 'filename': 'Final Review.pdf'}\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    print(f\"Example chunk (from{chunks[0].metadata[\"filename\"]}):\\n\")\n",
    "    preview_chunk = chunks[0].page_content[:300] + \"...\" if len(chunks[0].page_content) > 300 else chunks[0].page_content\n",
    "    print(preview_chunk)\n",
    "    print(\"\\nChunk metadata:\", chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. CREATE VECTOR STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and saved to faiss_index\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding = embeddings)\n",
    "vectorstore.save_local(FAISS_INDEX_PATH)\n",
    "print(f\"Vector store created and saved to {FAISS_INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. RETRIEVER FOR FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_filter(doc_type):\n",
    "    def filter_func(doc):\n",
    "        return doc.metadata.get(\"source_type\") == doc_type\n",
    "    return filter_func\n",
    "\n",
    "def create_retriever(doc_type = None):\n",
    "    vs = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    if doc_type:\n",
    "        base_retriever = vs.as_retriever(search_kwargs={\"k\": 10})\n",
    "        def filtered_retriever(query):\n",
    "                docs = base_retriever.invoke(query)\n",
    "                filtered_docs = [doc for doc in docs if doc.metadata.get(\"source_type\") == doc_type]\n",
    "                return filtered_docs[:4]\n",
    "        return filtered_retriever\n",
    "    else:\n",
    "        return vs.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. SET UP LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM setup completed\n"
     ]
    }
   ],
   "source": [
    "def setup_llm():\n",
    "    return HuggingFaceHub(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",  # You can change to another model\n",
    "        model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1024, \"n_ctx\": 2048, \"verbose\": True},\n",
    "        huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    )\n",
    "\n",
    "# Try to setup the LLM\n",
    "try:\n",
    "    llm = setup_llm()\n",
    "    print(\"LLM setup completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. CREATE RAG CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are an algorithms teaching assistant for a computer science class.\n",
    "Answer the question based only on the following context from class materials:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "When answering:\n",
    "1. Be thorough and explain concepts clearly.\n",
    "2. Use examples to illustrate complex algorithms when appropriate.\n",
    "3. Include time and space complexity analysis when relevant.\n",
    "4. If the answer isn't fully contained in the context, say so rather than making up information.\n",
    "5. Don't hallucinate.\n",
    "\n",
    "Your final output should include only your final answer preceded by 'Answer:'.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever = create_retriever()\n",
    "\n",
    "def create_rag_chain(retriever_func):\n",
    "    if callable(retriever_func) and not hasattr(retriever_func, 'invoke'):\n",
    "        retriever_chain = RunnablePassthrough() | retriever_func | format_docs\n",
    "    else:\n",
    "        retriever_chain = retriever_func | format_docs\n",
    "    \n",
    "    chain = (\n",
    "        {\"context\": retriever_chain, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "rag_chain = create_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. SMART DOCUMENT SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_query(question):\n",
    "    question_lower = question.lower()\n",
    "    if any(term in question_lower for term in [\"solution\", \"answer\", \"solved\", \"how to solve\"]):\n",
    "        print(\"Using primarily homework solutions for this query...\")\n",
    "        custom_retriever = create_retriever(\"homework_solution\")\n",
    "    elif any(term in question_lower for term in [\"lecture\", \"class\", \"taught\", \"professor\"]):\n",
    "        print(\"Using primarily lecture materials for this query...\")\n",
    "        custom_retriever = create_retriever(\"lecture\")\n",
    "    elif any(term in question_lower for term in [\"exam\", \"test\", \"quiz\", \"midterm\", \"final\"]):\n",
    "        print(\"Using primarily exam materials for this query...\")\n",
    "        custom_retriever = create_retriever(\"exam\")\n",
    "    elif any(term in question_lower for term in [\"homework\", \"assignment\", \"problem set\"]):\n",
    "        print(\"Using primarily homework materials for this query...\")\n",
    "        custom_retriever = create_retriever(\"homework\")\n",
    "    else:\n",
    "        print(\"Using all course materials for this query...\")\n",
    "        custom_retriever = create_retriever()\n",
    "    temp_chain = create_rag_chain(custom_retriever)\n",
    "    return temp_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. TEST RAG SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all course materials for this query...\n",
      "Human: You are an algorithms teaching assistant for a computer science class.\n",
      "Answer the question based only on the following context from class materials:\n",
      "\n",
      "Mergesort implementation\n",
      "tnputTFFwistFLFofFn elementsFfromFaFtotallyForderedFuniverseTF\n",
      "zutputTFF—heFnFelementsFinFascendingForderT\n",
      "c\n",
      "MERGE-SORT(L)\n",
      "\n",
      "8\n",
      "Divide and conquer: merge sort\n",
      "\n",
      "7\n",
      "Divide and conquer: merge sort\n",
      "\n",
      "Mergesort\n",
      "独”ecursivelyFsortFleftFhalfTF\n",
      "独”ecursivelyFsortFrightFhalfTF\n",
      "独xergeFtwoFhalvesFtoFmakeFsortedFwholeT\n",
      "a\n",
      "l r s t w x z ” … —\n",
      "merge results\n",
      "l w r z ” t — s x …\n",
      "input\n",
      "t — s x …l r w z ”\n",
      "sort left half\n",
      "s t x … —\n",
      "sort right half\n",
      "l r w z ”\n",
      "\n",
      "Question: Explain how merge-sort works\n",
      "\n",
      "When answering:\n",
      "1. Be thorough and explain concepts clearly.\n",
      "2. Use examples to illustrate complex algorithms when appropriate.\n",
      "3. Include time and space complexity analysis when relevant.\n",
      "4. If the answer isn't fully contained in the context, say so rather than making up information.\n",
      "5. Don't hallucinate.\n",
      "\n",
      "Your final output should include only your final answer preceded by 'Answer:'.\n",
      "\n",
      "Answer:\n",
      "Merge-sort is a divide and conquer algorithm. It works by recursively sorting the left and right halves of an array, and then merging the sorted halves back together. The merging process combines two sorted halves into a single sorted array. This process continues until the base case, which is an array of size one or less, is reached. At this point, the array is already sorted, so no further merging is necessary. The time complexity of merge-sort is O(n log n) in the average and worst cases, and the space complexity is O(n) due to the need to store the temporary merge results.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Explain how merge-sort works\"\n",
    "try:\n",
    "    answer = smart_query(test_question)\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nYou may need to download a language model or start text-generation-webui.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. CREATE DISCORD BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgorithmsBot(commands.Bot):\n",
    "    def __init__(self):\n",
    "        intents = discord.Intents.default()\n",
    "        intents.message_content = True\n",
    "        super().__init__(command_prefix=\"!\", intents=intents)\n",
    "\n",
    "    async def on_ready(self):\n",
    "        print(f'{self.user} has connected to Discord!')\n",
    "        print(f'Bot is in {len(self.guilds)} servers')\n",
    "\n",
    "    async def setup_hook(self):\n",
    "        await self.add_cog(AlgorithmsCog(self))\n",
    "\n",
    "class AlgorithmsCog(commands.Cog):\n",
    "    def __init__(self, bot):  # Accept bot parameter\n",
    "        self.bot = bot\n",
    "        self.retriever = create_retriever()\n",
    "        # Create the RAG chain\n",
    "        self.rag_chain = create_rag_chain(self.retriever)\n",
    "\n",
    "    @commands.command(name='algo')\n",
    "    async def algo_command(self, ctx, *, question):\n",
    "        async with ctx.typing():\n",
    "            try:\n",
    "                # Invoke the chain to get the raw output.\n",
    "                raw_output = self.rag_chain.invoke(question)\n",
    "                \n",
    "                # Post-process the output to only include content after \"Answer:\"\n",
    "                if \"Answer:\" in raw_output:\n",
    "                    filtered_answer = raw_output.split(\"Answer:\")[-1].strip()\n",
    "                else:\n",
    "                    filtered_answer = raw_output.strip()\n",
    "                \n",
    "                # Build the final message displaying just the question and answer\n",
    "                response = f\"**Question:** {question}\\n\\n**Answer:** {filtered_answer}\"\n",
    "                \n",
    "                # Send the response in chunks if it is long\n",
    "                if len(response) > 1900:\n",
    "                    chunks = [response[i:i+1900] for i in range(0, len(response), 1900)]\n",
    "                    for chunk in chunks:\n",
    "                        await ctx.send(chunk)\n",
    "                else:\n",
    "                    await ctx.send(response)\n",
    "            except Exception as e:\n",
    "                await ctx.send(f\"Error: {str(e)}\")\n",
    "\n",
    "    @commands.command(name='sources')\n",
    "    async def source_filter_command(self, ctx, source_type, *, question):\n",
    "        valid_sources = [\"lecture\", \"homework\", \"solution\", \"exam\", \"all\"]\n",
    "        if source_type.lower() not in valid_sources:\n",
    "            await ctx.send(f\"Invalid source type. Use one of: {', '.join(valid_sources)}\")\n",
    "            return\n",
    "        \n",
    "        doc_type = None\n",
    "        if source_type.lower() == \"solution\":\n",
    "            doc_type = \"homework_solution\"\n",
    "        elif source_type.lower() != \"all\":\n",
    "            doc_type = source_type.lower()\n",
    "        \n",
    "        async with ctx.typing():\n",
    "            try:\n",
    "                custom_retriever = create_retriever(doc_type)\n",
    "                temp_chain = create_rag_chain(custom_retriever)\n",
    "                answer = temp_chain.invoke(question)\n",
    "                \n",
    "                source_display = \"all sources\" if source_type.lower() == \"all\" else f\"{source_type} materials\"\n",
    "                if len(answer) > 1850:\n",
    "                    chunks = [answer[i:i+1850] for i in range(0, len(answer), 1850)]\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        if i == 0:\n",
    "                            await ctx.send(f\"**Question:** {question}\\n**Source:** {source_display}\\n\\n{chunk}\")\n",
    "                        else:\n",
    "                            await ctx.send(chunk)\n",
    "                else:\n",
    "                    await ctx.send(f\"**Question:** {question}\\n**Source:** {source_display}\\n\\n{answer}\")\n",
    "            except Exception as e:\n",
    "                await ctx.send(f\"Error: {str(e)}\")\n",
    "\n",
    "    @commands.command(name='help_algo')\n",
    "    async def help_command(self, ctx):      \n",
    "        help_text = (\n",
    "            \"**Algorithms Bot Commands:**\\n\\n\"\n",
    "            \"`!algo [question]` - Ask any algorithms question\\n\"\n",
    "            \"`!sources [type] [question]` - Search only specific source types\\n\"\n",
    "            \"  - Valid types: lecture, homework, solution, exam, all\\n\"\n",
    "            \"`!help_algo` - Show this help message\\n\\n\"\n",
    "            \"**Examples:**\\n\"\n",
    "            \"`!algo How does quicksort work?`\\n\"\n",
    "            \"`!sources lecture What is dynamic programming?`\"\n",
    "        )\n",
    "        await ctx.send(help_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-08 23:14:57] [INFO    ] discord.client: logging in using static token\n",
      "[2025-04-08 23:14:58] [INFO    ] discord.gateway: Shard ID None has connected to Gateway (Session ID: dc81452e3f40b11ad36fe277ae52607d).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithms RAG Bot#0980 has connected to Discord!\n",
      "Bot is in 1 servers\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "bot = AlgorithmsBot()\n",
    "bot.run(DISCORD_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
